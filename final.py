import re
import torch
import jieba
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer


class EnhancedPinyinConverter:
    _model = None
    _tokenizer = None
    _streamer = None
    _context = ""  # ä¿å­˜è¿ç»­çš„ä¸­æ–‡ä¸Šä¸‹æ–‡

    @classmethod
    def initialize_model(cls, model_path="/Users/a29301/Desktop/DeepSeek-R1-Distill-Qwen-1.5B"):
        if cls._model is None or cls._tokenizer is None:
            print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½è½¬æ¢å¼•æ“...")
            cls._tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            cls._model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                device_map="auto",
                torch_dtype=torch.bfloat16
            )

            cls._streamer = TextStreamer(cls._tokenizer, skip_prompt=True, skip_special_tokens=True)

            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return cls._tokenizer, cls._model

    @classmethod
    def _build_prompt(cls, pinyin_str):
        return f"""[æ™ºèƒ½æ‹¼éŸ³è½¬æ¢ç³»ç»Ÿ]

ä»»åŠ¡è¯´æ˜ï¼š
å°†è¿ç»­æ‹¼éŸ³å­—ç¬¦ä¸²ç²¾å‡†è½¬æ¢ä¸ºç¬¦åˆè¯­å¢ƒçš„ä¸­æ–‡æ–‡æœ¬ã€‚è¦æ±‚ï¼š
1. æ­£ç¡®åˆ‡åˆ†æ‹¼éŸ³ç‰‡æ®µ(æ— ç©ºæ ¼åˆ†éš”ï¼‰
2. ç²¾ç¡®è¿›è¡ŒéŸ³èŠ‚åˆ‡åˆ†(å¦‚"zhongguoren" â†’ "zhong guo ren")
3. å‡†ç¡®å¤„ç†å¤šéŸ³å­—(å¦‚"è¡ŒxÃ­ng/hÃ¡ng")
4. ä¿æŒè¯­ä¹‰è¿è´¯æ€§

è½¬æ¢è§„åˆ™ï¼š
- è¾“å…¥æ ¼å¼:çº¯å­—æ¯å­—ç¬¦ä¸²(å¦‚"woaini")
- è¾“å‡ºæ ¼å¼:å®Œæ•´è¯å¥
- ä¼˜å…ˆé€‰æ‹©å¸¸ç”¨è¯ç»„
- ä¿æŒåŸæ–‡é•¿åº¦ä¸€è‡´
- ä¸éœ€è¦ä»»ä½•å¤šä½™çš„è§£é‡Šè¯´æ˜,åªéœ€è¦ç»™å‡ºæœ€ç»ˆè½¬æ¢ç»“æœ

ç¤ºä¾‹:
è¾“å…¥: "zhongguoren"
è¾“å‡º: "ä¸­å›½äºº" 
è¾“å…¥: "yidongdianhua"
è¾“å‡º: "ç§»åŠ¨ç”µè¯" 
è¾“å…¥: "xianzai"
è¾“å‡º: "ç°åœ¨" 
è¾“å…¥: "hangkong"
è¾“å‡º: "èˆªç©º" 
è¾“å…¥: "nihaoshijie"
è¾“å‡º: "ä½ å¥½ä¸–ç•Œ" 
è¾“å…¥: "guangdongshenzhen"
è¾“å‡º: "å¹¿ä¸œæ·±åœ³"

ä¸Šä¸‹æ–‡å‚è€ƒï¼š
{cls._context.strip() if cls._context else "æ— "} 

å¾…è½¬æ¢è¾“å…¥ï¼š
{pinyin_str}

[è½¬æ¢ç»“æœ]
"""

    @classmethod
    def _generate_response(cls, pinyin_str):
        tokenizer, model = cls.initialize_model(model_path="D:/qwen_0.6b")

        generation_config = {
            "max_new_tokens": 128,  # å‡å°‘ç”Ÿæˆé•¿åº¦
            "temperature": 0.1,
            "top_p": 0.9,
            "repetition_penalty": 1.3,  # å¢å¼ºé‡å¤æƒ©ç½š
            "do_sample": True,
            "pad_token_id": tokenizer.eos_token_id,
            "no_repeat_ngram_size": 3  # é¿å…é‡å¤
        }

        prompt = cls._build_prompt(pinyin_str)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(**inputs,
                                    streamer=cls._streamer,
                                    **generation_config)

        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # return full_response
        return full_response.split("[è½¬æ¢ç»“æœ]")[-1].strip()  # å…³é”®ä¿®æ­£ç‚¹

    @staticmethod
    def _post_process(sentence):
        # æ¸…é™¤æ®‹ç•™ç¬¦å·
        sentence = re.sub(r"^[ï¼šã€]", "", sentence)
        # ç¡®ä¿ä»¥æ ‡ç‚¹ç»“å°¾
        if not re.search(r"[ã€‚ï¼ï¼Ÿ]$", sentence):
            sentence += "ã€‚"
        return sentence

    @classmethod
    def convert(cls, pinyin_str):
        try:
            print(f"\nğŸ” æ­£åœ¨åˆ†æè¾“å…¥ï¼š{pinyin_str}")
            raw_response = cls._generate_response(pinyin_str)
            print("ğŸ¯ åŸå§‹å“åº”è§£æä¸­...")

            # ç›´æ¥ä½¿ç”¨å¤„ç†åçš„å“åº”
            final_output = cls._post_process(raw_response)
            # å°†å½“å‰è½¬æ¢ç»“æœåŠ å…¥ä¸Šä¸‹æ–‡
            if cls._context:
                cls._context += final_output
            else:
                cls._context = final_output
            # æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¿ç•™ä¸è¶…è¿‡200å­—ç¬¦ï¼‰
            if len(cls._context) > 200:
                cls._context = cls._context[-200:]
            return final_output
        except Exception as e:
            print(f"âš ï¸ è½¬æ¢å¼‚å¸¸ï¼š{str(e)}")
            return "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•"
    @classmethod
    def clear_context(cls):
        cls._context = ""


def main():
    parser = argparse.ArgumentParser(description="æ™ºèƒ½æ‹¼éŸ³è½¬æ¢ç³»ç»Ÿ")
    parser.add_argument("input", nargs="?",default="tiandi", help="è¾“å…¥æ‹¼éŸ³å­—ç¬¦ä¸²")
    args = parser.parse_args()

    print("\n=== æ™ºèƒ½æ‹¼éŸ³è½¬æ¢ç³»ç»Ÿ v2.2 ===")

    input_str = args.input if args.input else input("ğŸ“ è¯·è¾“å…¥æ‹¼éŸ³å­—ç¬¦ä¸²ï¼š")

    if not re.match(r"^[a-zA-Z]+$", input_str):
        print("âŒ è¾“å…¥åŒ…å«éæ³•å­—ç¬¦ï¼åªæ¥å—çº¯å­—æ¯")
        return

    print("\nâ³ æ­£åœ¨è½¬æ¢ä¸­ï¼Œè¯·ç¨å€™...")
    result = EnhancedPinyinConverter.convert(input_str)

    print("\nâœ… è½¬æ¢ç»“æœï¼š")
    print("â”" * 40)
    print(result)
    print("â”" * 40)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ“ä½œå·²å–æ¶ˆ")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}")
